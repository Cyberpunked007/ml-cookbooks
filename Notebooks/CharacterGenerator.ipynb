{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFEiDY4PiHzaCy5jt0ouw1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Play Generator"],"metadata":{"id":"YIY1F9ABXsZj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tu0ElY6WvKe","executionInfo":{"status":"ok","timestamp":1674997542273,"user_tz":480,"elapsed":12,"user":{"displayName":"Ayodeji Odesile","userId":"14545298934971490071"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66928d2b-9d6b-4029-9d40-fcea953d3cc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}],"source":["%tensorflow_version 2.x"]},{"cell_type":"code","source":["from keras.preprocessing import sequence\n","import keras\n","import tensorflow as tf\n","import os\n","import numpy as np"],"metadata":{"id":"K_pinc7lX0LS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset here is going to be Romeo and Juliet, but, if we would like, we could use any, even our own write ups and poems."],"metadata":{"id":"rI3t9H-4Xqjm"}},{"cell_type":"code","source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"metadata":{"id":"dBLVtahPY5LZ","executionInfo":{"status":"ok","timestamp":1674997742718,"user_tz":480,"elapsed":560,"user":{"displayName":"Ayodeji Odesile","userId":"14545298934971490071"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"145184c3-5dca-4cbb-9eab-d5588e019bf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["# Loading Your Own Data\n","\n"],"metadata":{"id":"beV_r0jAZkhP"}},{"cell_type":"code","source":["from google.colab import files\n","path_to_file = list(files.upload().keys())[0]  # note, whatever file you choose, should be a txt file."],"metadata":{"id":"72zQgEVoZxWV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reading the File"],"metadata":{"id":"A3GLiOo8aU-Q"}},{"cell_type":"code","source":["text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n","print('Length of text: {} characters'.format(len(text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ICsSwSMKabge","executionInfo":{"status":"ok","timestamp":1674997747327,"user_tz":480,"elapsed":359,"user":{"displayName":"Ayodeji Odesile","userId":"14545298934971490071"}},"outputId":"0a7945c4-5861-413d-d1ef-65aa04b657a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of text: 1115394 characters\n"]}]},{"cell_type":"markdown","source":["# Encoding\n","\n","Carrying out some text preprocessing."],"metadata":{"id":"tEjNPehAc4HK"}},{"cell_type":"code","source":["vocab = sorted(set(text)) # This will basically sort out all the unique characters in the play\n","char_to_index = {u:i for i, u in enumerate(vocab)}\n","index_to_char = np.array(vocab)\n","\n","def text_to_int(text):\n","  return np.array([char_to_index[c] for c in text]) #for each character, which represents a key in the dictionary, it will return its value(index) after converting them all to an array\n","\n","text_as_int = text_to_int(text)"],"metadata":{"id":"rF8KmU65dE6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A simple functin to convert from the integer encoding back to the text"],"metadata":{"id":"HeVP988kfGIg"}},{"cell_type":"code","source":["def int_to_text(ints):\n","  try:\n","    ints = ints.numpy() #Converting to a numpy array, if this isn't already one\n","  except:\n","    pass # if it is, then don't do anything\n","  return ''.join(index_to_char[ints])"],"metadata":{"id":"mqpv7uQ0fK1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating Training Examples\n","\n","We have to split the text into shorter sequences to pass to the model as training examples, since our model is going to be returning 1 character per training example.\n","\n","This training example will use a seq_length sequence as input and outpit, where the one at the output is the original sequence shifted one letter to the right.\n","\n","The first step will be to create a stream of characters from our text data"],"metadata":{"id":"RGLE0UZ3g-D3"}},{"cell_type":"code","source":["seq_length = 100\n","examples_per_epoch = len(text)//(seq_length + 1) #Plus 1 because the output sequence will be shifted by 1 to the right, hence it is actually 101 characters used\n","\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)   # make the string a stream of characters"],"metadata":{"id":"LcLIiQtXiS3q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we turn the stream of characters into the batches of desired length"],"metadata":{"id":"aUWqgF7MjcrV"}},{"cell_type":"code","source":["sequences = char_dataset.batch(seq_length + 1, drop_remainder = True) # any remainder after the division that is not enough to be batched, is dropped"],"metadata":{"id":"Nn-h0sw2jivM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The sequences of length 101 will be split into input and output"],"metadata":{"id":"buwm4jrIkQ93"}},{"cell_type":"code","source":["def split_input_target(full):\n","  input_text = full[:-1]\n","  target_text = full[1:]\n","  return input_text, target_text\n","\n","dataset = sequences.map(split_input_target) # This will apply the function to every sequence"],"metadata":{"id":"NyLTgAy3kZjq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we make our training batches"],"metadata":{"id":"c_VJec9hmSif"}},{"cell_type":"code","source":["BATCH_SIZE = 64\n","VOCAB_SIZE = len(vocab)\n","EMBEDDING_DIMENSIONS = 256 # This describes the vector dimensions in the embedding layer\n","RNN_UNITS = 1024\n","\n","BUFFER_SIZE = 10000\n","data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)"],"metadata":{"id":"buKQ-umgmYM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building the Model\n","\n","The dense layer in our model will contain a node for each unique character in our training data. The dense layer will give us a probability distribution over each node."],"metadata":{"id":"JHpXGC_w22zl"}},{"cell_type":"code","source":["def build_model(vocab_size, embedding_dimensions, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","                               tf.keras.layers.Embedding(vocab_size, embedding_dimensions, \n","                                                         batch_input_shape = [batch_size, None]),\n","                               tf.keras.layers.LSTM(rnn_units, return_sequences = True, #This is so that the model will return the output at every timestamp, not just the final output\n","                                                    stateful = True, recurrent_initializer = 'glorot_uniform'),  #LSTM is long short term memory\n","                               tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model\n","\n","model = build_model(VOCAB_SIZE, EMBEDDING_DIMENSIONS, RNN_UNITS, BATCH_SIZE)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtSUSayq3gNA","executionInfo":{"status":"ok","timestamp":1674997773967,"user_tz":480,"elapsed":387,"user":{"displayName":"Ayodeji Odesile","userId":"14545298934971490071"}},"outputId":"9486010d-c7c0-4e6d-a51a-b7b2b1b14b2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (64, None, 256)           16640     \n","                                                                 \n"," lstm (LSTM)                 (64, None, 1024)          5246976   \n","                                                                 \n"," dense (Dense)               (64, None, 65)            66625     \n","                                                                 \n","=================================================================\n","Total params: 5,330,241\n","Trainable params: 5,330,241\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# Creating a Loss Function\n","\n","We need to create a loss function, because of the type of predictions our model will be making. First, we are looking at an input and the output from our untrained model, just to see what the model is giving us"],"metadata":{"id":"gQz-6OOM6ZUZ"}},{"cell_type":"code","source":["for input_example_batch, target_example_batch in data.take(1):\n","  example_batch_predictions = model(input_example_batch) # Ask our model for a prediction on our first batch.\n","  print(example_batch_predictions.shape, ' = (batch_size, sequence_length, vocab_size)')\n","\n","pred = example_batch_predictions[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5oPghhQIAkgc","executionInfo":{"status":"ok","timestamp":1674997782815,"user_tz":480,"elapsed":5661,"user":{"displayName":"Ayodeji Odesile","userId":"14545298934971490071"}},"outputId":"de02834f-89ce-4021-9290-3523599ddc5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 65)  = (batch_size, sequence_length, vocab_size)\n"]}]},{"cell_type":"code","source":["# If we want to determine the predicted character, we need to sample the output distribution (pick a value based on probability distribution)\n","sampled_indices = tf.random.categorical(pred, num_samples = 1)  #We're sampling\n","\n","# Now, we reshape the array and convert all the integers to numbers to see the actual characters\n","sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n","predicted_chars = int_to_text(sampled_indices)\n","\n","predicted_chars"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"3Rhowl65-tnH","executionInfo":{"status":"ok","timestamp":1674997785705,"user_tz":480,"elapsed":608,"user":{"displayName":"Ayodeji Odesile","userId":"14545298934971490071"}},"outputId":"4117004a-7d56-432c-e75a-ff588fbf1552"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nIWitI:H-IakG,gsX UnxkRSBXmXsNyhgcMzb'VqXU,aTaEy?bfKFQRuo,&fMSPUt:QUYamweq$z;yeJ.V:K$UxW3FgNt -,!koA\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["Now we create a loss function that can compare the output to the expected output and give us some numeric value representing how close the two were"],"metadata":{"id":"Rg8m7qKDE-Bm"}},{"cell_type":"code","source":["def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"],"metadata":{"id":"p7dvWixnFNga"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Compiling the Model"],"metadata":{"id":"nMC5CwQ7cWls"}},{"cell_type":"code","source":["model.compile(optimizer = 'adam', loss = loss)"],"metadata":{"id":"BL4IjmIqchAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating Checkpoints\n","\n","Now we are going to setup and configure our model to save checkpoints as it trains. This will allow us to load our model from a checkpoint and continue training it."],"metadata":{"id":"PjiSr06ueXyD"}},{"cell_type":"code","source":["# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'   \n","#Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath = checkpoint_prefix,\n","    save_weights_only = True)"],"metadata":{"id":"rLQVhlBLeuv6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"OiUkABoVfkPg"}},{"cell_type":"code","source":["history = model.fit(data, epochs =100, callbacks = [checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWc4hbA-gWnj","executionInfo":{"status":"ok","timestamp":1674999717474,"user_tz":480,"elapsed":1658913,"user":{"displayName":"Ayodeji Odesile","userId":"14545298934971490071"}},"outputId":"9e4c2aee-6151-4b52-ba79-6ef82e9f9203"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","172/172 [==============================] - 15s 68ms/step - loss: 2.5771\n","Epoch 2/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.8658\n","Epoch 3/100\n","172/172 [==============================] - 13s 71ms/step - loss: 1.6189\n","Epoch 4/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.4898\n","Epoch 5/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.4128\n","Epoch 6/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.3575\n","Epoch 7/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.3123\n","Epoch 8/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.2723\n","Epoch 9/100\n","172/172 [==============================] - 13s 68ms/step - loss: 1.2361\n","Epoch 10/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.1988\n","Epoch 11/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.1623\n","Epoch 12/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.1232\n","Epoch 13/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.0837\n","Epoch 14/100\n","172/172 [==============================] - 13s 68ms/step - loss: 1.0426\n","Epoch 15/100\n","172/172 [==============================] - 13s 69ms/step - loss: 1.0009\n","Epoch 16/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.9577\n","Epoch 17/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.9151\n","Epoch 18/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.8744\n","Epoch 19/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.8343\n","Epoch 20/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.7970\n","Epoch 21/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.7623\n","Epoch 22/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.7287\n","Epoch 23/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.6990\n","Epoch 24/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.6702\n","Epoch 25/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.6459\n","Epoch 26/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.6229\n","Epoch 27/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.6029\n","Epoch 28/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.5854\n","Epoch 29/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.5699\n","Epoch 30/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.5538\n","Epoch 31/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.5398\n","Epoch 32/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.5287\n","Epoch 33/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.5177\n","Epoch 34/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.5074\n","Epoch 35/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.4989\n","Epoch 36/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4898\n","Epoch 37/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4809\n","Epoch 38/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4743\n","Epoch 39/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.4708\n","Epoch 40/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4643\n","Epoch 41/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4597\n","Epoch 42/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4557\n","Epoch 43/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4501\n","Epoch 44/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4459\n","Epoch 45/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4418\n","Epoch 46/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4381\n","Epoch 47/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4361\n","Epoch 48/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4340\n","Epoch 49/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.4313\n","Epoch 50/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4311\n","Epoch 51/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4277\n","Epoch 52/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4262\n","Epoch 53/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4218\n","Epoch 54/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4191\n","Epoch 55/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.4173\n","Epoch 56/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4149\n","Epoch 57/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4146\n","Epoch 58/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4144\n","Epoch 59/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4129\n","Epoch 60/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4106\n","Epoch 61/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.4097\n","Epoch 62/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4077\n","Epoch 63/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.4073\n","Epoch 64/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4044\n","Epoch 65/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4058\n","Epoch 66/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4038\n","Epoch 67/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4044\n","Epoch 68/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4031\n","Epoch 69/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4027\n","Epoch 70/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4003\n","Epoch 71/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.4005\n","Epoch 72/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3996\n","Epoch 73/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.3989\n","Epoch 74/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.4012\n","Epoch 75/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3986\n","Epoch 76/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3989\n","Epoch 77/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3977\n","Epoch 78/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3960\n","Epoch 79/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3961\n","Epoch 80/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3949\n","Epoch 81/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3923\n","Epoch 82/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3969\n","Epoch 83/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.3966\n","Epoch 84/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3952\n","Epoch 85/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3953\n","Epoch 86/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3945\n","Epoch 87/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3947\n","Epoch 88/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3932\n","Epoch 89/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3935\n","Epoch 90/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.3933\n","Epoch 91/100\n","172/172 [==============================] - 13s 70ms/step - loss: 0.3939\n","Epoch 92/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3933\n","Epoch 93/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3915\n","Epoch 94/100\n","172/172 [==============================] - 13s 71ms/step - loss: 0.3928\n","Epoch 95/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3925\n","Epoch 96/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3894\n","Epoch 97/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3916\n","Epoch 98/100\n","172/172 [==============================] - 13s 68ms/step - loss: 0.3914\n","Epoch 99/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3917\n","Epoch 100/100\n","172/172 [==============================] - 13s 69ms/step - loss: 0.3935\n"]}]},{"cell_type":"markdown","source":["# Loading the Model\n","\n","We'll rebuild the model from a checkpoint using a batch_size of 1 so that we can feed one piece of text to the model and have it make a prediction"],"metadata":{"id":"iby2_jAehDHd"}},{"cell_type":"code","source":["model = build_model(VOCAB_SIZE, EMBEDDING_DIMENSIONS, RNN_UNITS, batch_size = 1)"],"metadata":{"id":"SEOpyh8-hVkG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the model is finished, we can find the latest checkpoint that stores the models weights using the following line"],"metadata":{"id":"28RKxXjPiXCj"}},{"cell_type":"code","source":["model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))"],"metadata":{"id":"8pZg7T8oilpP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can load any checkpoint we want to by specifying the exact file to load"],"metadata":{"id":"xa92JtVhi0kE"}},{"cell_type":"code","source":["checkpoint_num = 10\n","model.load_weights(tf.train.load_checkpoint('./training_checkpoints/ckpt_' + str(checkpoint_num)))\n","model.build(tf.TensorShape([1, None]))"],"metadata":{"id":"d4eGQ_exi8qt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate Text"],"metadata":{"id":"pfaT3cbgmDah"}},{"cell_type":"code","source":["def generate_text(model, start_string):\n","  num_generate = 1000 #Number of characters to generate\n","\n","  input_eval = text_to_int(start_string)\n","  input_eval = tf.expand_dims(input_eval, 0)  # This adds an extra nested list on it, since thats what the model is expecting\\\n","\n","  text_generated = [] #empty string to store our results\n","\n","  # Low temperatures results in more predictible text, while higher ones result in surprising text\n","  # Experiment to find the best setting\n","  temperature = 1.0\n","\n","  model.reset_states() # While training, the model will have saved its states, we don't want that so we clear those saved states\n","\n","  for i in range(num_generate):\n","    predictions = model(input_eval)\n","    predictions = tf.squeeze(predictions, 0) # removes the extra dimension we added\n","\n","    predictions = predictions/temperature\n","    predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1, 0].numpy() # Sampling\n","\n","    input_eval = tf.expand_dims([predicted_id], 0) # We are passing the predicted character as the next input into the model\n","\n","    text_generated.append(index_to_char[predicted_id]) # appending the output to the empty list\n","\n","  return(start_string + ''.join(text_generated))\n","  "],"metadata":{"id":"plOLmk1umJxi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = input('Type a starting string: ')\n","print(generate_text(model, inp))"],"metadata":{"id":"P_FUu6wUqYGy"},"execution_count":null,"outputs":[]}]}